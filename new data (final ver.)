
# coding: utf-8

# In[166]:

get_ipython().magic('matplotlib inline')
import matplotlib.pyplot as plt
import matplotlib
matplotlib.style.use('ggplot')
import statsmodels as sm
from scipy.spatial.distance import cdist
import pandas as pd
from pandas import concat
from datetime import datetime
import glob
import os as os
import time
import numpy as np
from statsmodels.formula.api import ols
import seaborn as sb


# In[12]:

#Function for performing time series analysis for Date aggregated data. That is to say the data frame should have only
#have columns "Date", "Visitor_Type", "Count", and "Sample". This function is used to look at global web browsing patterns
#as a whole, as time progresses.
#Returns
def tsa_by_date(by_date_df):
    from statsmodels.tsa.seasonal import seasonal_decompose
    copy = by_date_df.copy()

    #Separate Visitor_type
    copy_a = copy[copy["Visitor_type"]=="visitors-total"].reset_index() 
    copy_b = copy[copy["Visitor_type"]=="visitors-unique"].reset_index()
    
    #"Ratio" is the normalized count
    copy_a["Ratio"] = copy_a["Count"]/copy_a["Sample"]
    copy_b["Ratio"] = copy_b["Count"]/copy_b["Sample"]
    
    #seasonal_decompose is much easier to work with when data in array form
    a = np.array(copy_a["Ratio"])
    b = np.array(copy_b["Ratio"])
    
    #seasonal_decompose: freq=7 means cyclic behavior has period 7
    #                    creates NA values because needs several points to determine start of trend, 
    #                    two_sided=False will place NAs at the earliest time, default two_sided=True places at both ends               
    model_a = seasonal_decompose(a, model="additive", filt=None, freq=7, two_sided=False)
    model_b = seasonal_decompose(b, model="additive", filt=None, freq=7, two_sided=False)

    #normalize residuals by dividing residuals by model values
    copy_a["Norm_res"] = np.absolute(model_a.resid/a)
    copy_b["Norm_res"] = np.absolute(model_b.resid/b)
    
    std_a = copy_a["Norm_res"].std()
    std_b = copy_b["Norm_res"].std()
    
    #SCORE: normalized residuals divided by sd of normalized residuals
    copy_a["Res/std"] = copy_a["Norm_res"]/copy_a["Norm_res"].std()
    copy_b["Res/std"] = copy_b["Norm_res"]/copy_b["Norm_res"].std()
    
    return_df = pd.concat([copy_a, copy_b], ignore_index=True).iloc[:,1:]
    
    return(return_df)


# In[14]:

def tsa_by_site(df_by_site):
    from statsmodels.tsa.seasonal import seasonal_decompose
    
    copy = df_by_site.copy()
    req_len = len(copy["Date"].unique())
    unique_sites = copy["Site"].unique()
    frames = []
    #stores sd of normalized residuals for tot and uni for each site, key is site name
    my_dict = {}
    
    #Same idea as tsa_by_date but now we loop through each site 
    for site in unique_sites:
        copy_subset = copy[copy["Site"] == site]
        
        #This line is very critical. Since very small sites do not have data for each day, seasonal_decompose will fail
        #if called. We skip these sites to avoid error and because there is not enough data to separate noise from trend
        if len(copy_subset) < req_len:
            continue

        copy_subset_a = copy_subset[copy_subset["Visitor_type"]=="visitors-total"].reset_index()
        copy_subset_b = copy_subset[copy_subset["Visitor_type"]=="visitors-unique"].reset_index()

        copy_subset_a["Ratio"] = copy_subset_a["Count"]/copy_subset_a["Sample"]
        copy_subset_b["Ratio"] = copy_subset_b["Count"]/copy_subset_b["Sample"]

        a = np.array(copy_subset_a["Ratio"])
        b = np.array(copy_subset_b["Ratio"])

        model_a = seasonal_decompose(a, model="additive", filt=None, freq=7, two_sided=False)
        model_b = seasonal_decompose(b, model="additive", filt=None, freq=7, two_sided=False)

        copy_subset_a["Norm_res"] = np.absolute(model_a.resid/a)
        copy_subset_b["Norm_res"] = np.absolute(model_b.resid/b)
        
        std_a = copy_subset_a["Norm_res"].std()
        std_b = copy_subset_b["Norm_res"].std()
        
        copy_subset_a["Res/std"] = copy_subset_a["Norm_res"]/std_a
        copy_subset_b["Res/std"] = copy_subset_b["Norm_res"]/std_b
        
        #Add new site datafram to frames
        frames += [pd.concat([copy_subset_a, copy_subset_b], ignore_index=True).iloc[:,1:]]
        #Add new site entry into my_dict
        my_dict[site] = (std_a, std_b, np.nanmean(a), np.nanmean(b))
    
    #Stack all dataframes in frames
    return_df = pd.concat(frames)
    return((return_df, my_dict))


# In[3]:

path = r'C:\new_data'

all_files = glob.glob(os.path.join(path, "*"))

types = {"Date":'category',
        "Site":'category',
        "URL":'category', 
        "Country":'category', 
        "Platform":'category',
        "Click_type": 'category',
        "Type":'category',
        "Count":'float32', 
        "Sample":'float32'}

df = pd.concat([pd.read_csv(f,
                            sep='/',
                            names = ["Date", "Site", "URL", "Country", "Platform", "Click_type", "Visitor_type", "Count", "Sample"],
                            dtype = types) for f in all_files], 
                            ignore_index=True)

df.columns = ["Date", "Site", "URL", "Country", "Platform", "Click_type", "Visitor_type", "Count", "Sample"]
df_PC = df[df.Platform == 'PC'].drop('Platform', axis=1)

df_count = (df_PC.groupby(["Date", "Site", "Click_type", "Visitor_type"])["Count"].sum()).reset_index() #aggregate Country 

df_sample = (df_PC.groupby(["Date", "Country"])["Sample"].mean()).reset_index() #groupby Date and Country using mean because 
                                                                                #given country and date, Sample values are same
df_sample = (df_sample.groupby(["Date"])["Sample"].sum()).reset_index() #sum over all Country to get total daily Sample

df_clean = pd.merge(df_count, df_sample, on="Date", how="left") #cleaned data from which we will further subset


# In[388]:

######FOLLOWING ANALYSIS AGGREGATES CLICKTYPE######
f = {'Count':'sum', 'Sample':'mean'}
df_agg = (df_clean.groupby(["Date", "Site", "Visitor_type"]).agg(f)).reset_index()

date_site_agg = a[0]
date_site_agg_dict = a[1]
    
temp = pd.DataFrame.from_dict(date_site_agg_dict, orient='index')
temp.reset_index(level=0, inplace=True)
temp.columns = ["Site", "std_of_res_tot", "std_of_res_uni", "mean_ratio_tot", "mean_ratio_uni"]


# In[473]:

variance_cutoff = 0.2 #cutoff for normalized standard deviations (measure of size of of flucutations to volume).
                      #0.0 means the noise is almost non-existant.
                      #>1.0 means noise is just as large or larger than actual volume. 

temp["Variance_flag"] = "Good"
temp["Variance_flag"][temp["std_of_res_tot"]>variance_cutoff] = "High_variance"

#NOTE: We use all data points, including High_variance points to create model.
x = np.array(temp["mean_ratio_tot"])
y = np.array(temp["std_of_res_tot"])

from sklearn import linear_model   

#ln transform x and y
ln_x = np.log(x)
ln_y = np.log(y)

#linear regression on transformed variables
m,b = np.polyfit(ln_x, ln_y, deg=1)

#create model coordinates
x_plot = np.arange(min(x), max(x), 0.0001)
x_ln_plot = np.arange(min(ln_x), max(ln_x), 0.0001)

model_y = np.exp(b) * x_plot**m
transformed_model_y = m*x_ln_plot + b

#plot to confirm transformationed data linear
plt.figure(num=None, figsize=(6, 6), dpi=80, facecolor='w', edgecolor='k')
plt.scatter(ln_x, ln_y, s=2, color='steelblue')   
plt.plot(x_ln_plot, transformed_model_y, color='orange')   
plt.title("Total: ln(std_resid) vs ln(volume)", fontsize=16)
plt.xlabel("ln(Normalized Count)")
plt.ylabel("ln(Normalized Sd of Residuals)")
plt.show()   

#plot model overlaid with original data
plt.figure(num=None, figsize=(6, 6), dpi=80, facecolor='w', edgecolor='k')
plt.scatter(x, y, s=2, color='steelblue')   
plt.plot(x_plot, model_y, color='orange')
plt.xlim(xmin=0, xmax=0.5)
plt.ylim(ymin=0, ymax=0.5)
plt.title("Total: std_resid vs volume", fontsize=16)
plt.xlabel("Normalized Count")
plt.ylabel("Normalized Sd of Residuals")
plt.show()


# In[476]:

#We now set High_variance points to 0. Their extreme values result in very high euclidean distances between data and model.
variance_cutoff_index = y < variance_cutoff #array of bools to indicate whether above or below variance_cutoff
prediction = (np.exp(b) * x**m) * variance_cutoff_index
x_1 = x * variance_cutoff_index
y_1 = y * variance_cutoff_index

y_above = (prediction > y_1)*prediction + (prediction <= y_1)*y_1
                                                                   #If the prediction is below the fitted model, 
                                                                   #we set the prediction_mod=y.
                                                                   #If the prediction is above the fitted model,
                                                                   #we set prediction_mod=prediction.
                                                                   #Thus, when we calculate euclidean distances, 
                                                                   #we will not have to worry about data points below the model.
data_coord = np.array([x_1, y_above]).T
                    
prediction = np.array([x_1, prediction]).T 

distances = cdist(data_coord, prediction_coord, 'euclidean') #Each row contains the euclidean distance between the 
                                                             #data point and all points on fitted model.
min_dist = distances.min(axis=1) #Minimum distance for each row

#NOTE: Since most of the points lie below the model and hence were aritifically given distance=0,
#the cutoff percentile must be set very high or else everyting above model will be flagged.
#The correct choice of cutoff_percentile will change depending on what the choice of variance_cutoff = 0.2

cutoff_percentile = 99
cutoff = np.percentile(min_dist, cutoff_percentile) #A non-percentile approach to determining cutoff will also work.
                                                    #If a non-percentile used, can potentially include High_variance points
    
#NOTE: The exclusion of High_variance points is more clear now. High_variance points correspond to high euclidean distances.
#The abundance of High_variance points will make it very difficult to determine a suitable cutoff_percentile beacuse 
#High_variance points will consitute the majority of the top 10% of distances.

flag_coord = np.array([(min_dist > cutoff)*x_1, (min_dist > cutoff)*y_1]).T

plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')
plt.scatter(x_1, y_1, s=2, color='steelblue', label="good") 
plt.scatter(flag_coord[:,0], flag_coord[:,1], s=2, color='red', label="high relative variance")
plt.plot(x_plot, model_y, linewidth=1, color='orange', label="model")
plt.scatter(temp["mean_ratio_tot"][temp["Variance_flag"]=="High_variance"], 
            temp["std_of_res_tot"][temp["Variance_flag"]=="High_variance"], s=2, color='green', label="high variance")
plt.title("Total: Flagging sites")
plt.legend(loc='upper right')
plt.xlim(xmin=0, xmax=0.5)
plt.ylim(ymin=0, ymax=0.5)
plt.xlabel("Normalized Count")
plt.ylabel("Normalized Sd of Residuals")
plt.show()

#it's hard to see the model at x=0, but numerical inspection will show that it is very close to data


# In[462]:

temp["Relative_variance_flag"] = "Good"
temp["Relative_variance_flag"][min_dist > cutoff] = "High_relative_variance"

site_table = temp[["Site", "Variance_flag", "Relative_variance_flag"]].copy() #THIS TABLE CONTAINS CLASSIFICATION FOR ALL SITES


# In[493]:

######FOLLOWING ANALYSIS FOR EACH CLICKTYPE######
#We break the df_clean into 8 individual data frames

titles = ["TABCHANGE", "REFRESH", "REGULAR", "SERVER_REDIRECT", "EQUAL_REFERER", "DYNAMIC_CHANGE", "AJAX"]
df_TABCHANGE       = df_clean[df_clean["Click_type"]=="TABCHANGE"]
df_REFRESH         = df_clean[df_clean["Click_type"]=="REFRESH"]
df_REGULAR         = df_clean[df_clean["Click_type"]=="REGULAR"]
df_SERVER_REDIRECT = df_clean[df_clean["Click_type"]=="SERVER_REDIRECT"]
df_EQUAL_REFERER   = df_clean[df_clean["Click_type"]=="EQUAL_REFERER"]
df_DYNAMIC_CHANGE  = df_clean[df_clean["Click_type"]=="DYNAMIC_CHANGE"]
df_AJAX            = df_clean[df_clean["Click_type"]=="AJAX"]

df_list = [df_TABCHANGE, df_REFRESH, df_REGULAR, df_SERVER_REDIRECT, df_EQUAL_REFERER, df_DYNAMIC_CHANGE, df_AJAX]


# In[494]:

#Perform tsa_by_site on each click_type data frame
site_scores_list = []
dict_list = []
for df_i in df_list:
    temp = tsa_by_site(df_i)
    site_scores_list += [temp[0]]
    dict_list += [temp[1]]


# In[495]:

n = len(df_list)
table = []
site_table = []

for i in range(n):
    temp = pd.DataFrame.from_dict(dict_list[i], orient='index').reset_index()
    temp.columns = ["Site","std_of_res_tot", "std_of_res_uni", "mean_ratio_tot", "mean_ratio_uni"]
    table += [temp]    

variance_cutoff = 0.2 

for i in range(n):
    table[i]["Variance_flag"] = "Good"
    table[i]["Variance_flag"][table[i]["std_of_res_tot"]>variance_cutoff] = "High_variance"

    x = np.array(table[i]["mean_ratio_tot"])
    y = np.array(table[i]["std_of_res_tot"])

    from sklearn import linear_model   

    #ln transform x and y
    ln_x = np.log(x)
    ln_y = np.log(y)

    #linear regression on transformed variables
    m,b = np.polyfit(ln_x, ln_y, deg=1)

    #create model coordinates
    x_plot = np.arange(min(x), max(x), 0.0001)
    x_ln_plot = np.arange(min(ln_x), max(ln_x), 0.0001)

    model_y = np.exp(b) * x_plot**m
    transformed_model_y = m*x_ln_plot + b

#     #plot to confirm transformationed data linear
#     plt.scatter(ln_x, ln_y, s=2, color='steelblue')   
#     plt.plot(x_ln_plot, transformed_model_y, color='orange')   
#     plt.title("Total: ln(std_resid) vs ln(volume)   (" + titles[i] + ")", fontsize=16)
#     plt.show()   

#     #plot model overlaid with original data
#     plt.scatter(x, y, s=2, color='steelblue')   
#     plt.plot(x_plot, model_y, color='orange')
#     plt.xlim(xmin=0, xmax=0.3)
#     plt.ylim(ymin=0, ymax=0.5)
#     plt.title("Total: std_resid vs volume   (" + titles[i] + ")", fontsize=16)
#     plt.show()

    #We now set High_variance points to 0. Their extreme values result in very high euclidean distances between data and model.
    variance_cutoff_index = y < variance_cutoff #array of bools to indicate whether above or below variance_cutoff
    prediction = (np.exp(b) * x**m) * variance_cutoff_index
    x_1 = x * variance_cutoff_index
    y_1 = y * variance_cutoff_index

    y_above = (prediction > y_1)*prediction + (prediction <= y_1)*y_1
                                                                       
    data_coord = np.array([x_1, y_above]).T
    prediction_coord = np.array([x_1, prediction]).T 

    distances = cdist(data_coord, prediction_coord, 'euclidean') #Each row contains the euclidean distance between the 
                                                                 #data point and all points on fitted model.
    min_dist = distances.min(axis=1) #Minimum distance for each row

    #SET cutoff_percentile
    cutoff_percentile = 98
    cutoff = np.percentile(min_dist, cutoff_percentile) #A non-percentile approach to determining cutoff will also work.
                                                        #If a non-percentile used, can potentially include High_variance points

    flag_coord = np.array([(min_dist > cutoff)*x_1, (min_dist > cutoff)*y_1]).T

    #Plot classification of sites
    plt.figure(num=None, figsize=(6, 6), dpi=80, facecolor='w', edgecolor='k')
    plt.scatter(x_1, y_1, s=2, color='steelblue', label="good") 
    plt.scatter(flag_coord[:,0], flag_coord[:,1], s=2, color='red', label="high relative variance")
    plt.plot(x_plot, model_y, linewidth=1, color='orange', label="model")
    plt.scatter(table[i]["mean_ratio_tot"][table[i]["Variance_flag"]=="High_variance"], 
                table[i]["std_of_res_tot"][table[i]["Variance_flag"]=="High_variance"], s=2, color='green', label="high variance")
    plt.title("Total: Flagging sites   (" + titles[i] + ")")
    plt.legend(loc='upper right')
    plt.xlim(xmin=0, xmax=0.3)
    plt.ylim(ymin=0, ymax=0.5)
    plt.show()

    table[i]["Relative_variance_flag"] = "Good"
    table[i]["Relative_variance_flag"][min_dist > cutoff] = "High_relative_variance"
    
    temp = table[i][["Site", "Variance_flag", "Relative_variance_flag"]].copy()
    temp["CLick_type"] = titles[i]
    temp = temp[["Site", "CLick_type", "Variance_flag", "Relative_variance_flag"]]
    site_table += [temp]


# In[496]:

combined_site_table = pd.concat(site_table) 
abnormal_site_table = combined_site_table[(combined_site_table['Variance_flag']=='High_variance') | (combined_site_table['Relative_variance_flag']=='High_relative_variance')]



