
# coding: utf-8

# In[1]:

get_ipython().magic('matplotlib inline')
import matplotlib.pyplot as plt
import matplotlib
matplotlib.style.use('ggplot')
import statsmodels as sm
import pandas as pd
from pandas import concat
from datetime import datetime
import glob
import os as os
import time
import numpy as np
from statsmodels.formula.api import ols
import seaborn as sb


# In[ ]:

def group(df, keyword): #keyword: "Country", "Site"
   
    df1 = df[df.Type == 'visitors-total'] 
    df2 = df[df.Type == 'visitors-unique']
    #only choose the data that: 1. Platform is PC; 2. Given visitors type.
    
    df1_1 = df1.groupby(["Date", keyword])["Count"].sum()    #sum up all vis-tot counts on given day, grouped by 'keyword'.
    
    df2_1 = df2.groupby(["Date", keyword])["Count"].sum()    #sum up all vis-uni counts on given day, grouped by 'keyword'.
    
    df1_2 = df1.groupby(["Date", keyword])["Sample"].mean()    #keeep sample size (sample size do not need summation).
    
    #convert them to DataFrame format
    df1_1 = pd.DataFrame(df1_1).reset_index()
    df2_1 = pd.DataFrame(df2_1).reset_index()
    df1_2 = pd.DataFrame(df1_2).reset_index()
    
    df1_1 = df1_1.rename(columns={'Count': 'VisTot_Count'})
    df2_1 = df2_1.rename(columns={'Count': 'VisUni_Count'})
    
    df_grouped = pd.concat([df1_1, df2_1[['VisUni_Count']], df1_2[['Sample']]], axis = 1)
    
    df_grouped['Ratio_1'] = df_grouped['VisTot_Count'] / df_grouped['Sample']
    df_grouped['Ratio_2'] = df_grouped['VisUni_Count'] / df_grouped['Sample']
    # Add columns of ratio1 and ratio2 for grouped dataframe
    
    df_grouped = df_grouped.drop(['VisTot_Count', 'VisUni_Count', 'Sample'], axis = 1)
    
    return df_grouped


# In[4]:

path = r'C:\new_data'

all_files = glob.glob(os.path.join(path, "*"))

types = {"Date":'category',
        "Site":'category',
        "URL":'category', 
        "Country":'category', 
        "Platform":'category',
        "Click_type": 'category',
        "Type":'category',
        "Count":'float32', 
        "Sample":'float32'}

df = pd.concat([pd.read_csv(f,
                            sep='/',
                            names = ["Date", "Site", "URL", "Country", "Platform", "Click_type", "Visitor_type", "Count", "Sample"],
                            dtype = types) for f in all_files], 
                            ignore_index=True)

df.columns = ["Date", "Site", "URL", "Country", "Platform", "Click_type", "Visitor_type", "Count", "Sample"]
df_PC = df[df.Platform == 'PC'].drop('Platform', axis=1)


# In[24]:

df_count = (df_PC.groupby(["Date", "Site", "Click_type", "Visitor_type"])["Count"].sum()).reset_index() #aggregate Country 

df_sample = (df_PC.groupby(["Date", "Country"])["Sample"].mean()).reset_index() #groupby Date and Country using mean because given country and date, Sample values are same
df_sample = (df_sample.groupby(["Date"])["Sample"].sum()).reset_index() #sum over all Country to get daily Sample

df_clean = pd.merge(df_count, df_sample, on="Date", how="left") 


# In[172]:

titles = ["TABCHANGE", "REFRESH", "REGULAR", "SERVER_REDIRECT", "EQUAL_REFERER", "DYNAMIC_CHANGE", "AJAX"]
df_TABCHANGE       = df_clean[df_clean["Click_type"]=="TABCHANGE"]
df_REFRESH         = df_clean[df_clean["Click_type"]=="REFRESH"]
df_REGULAR         = df_clean[df_clean["Click_type"]=="REGULAR"]
df_SERVER_REDIRECT = df_clean[df_clean["Click_type"]=="SERVER_REDIRECT"]
df_EQUAL_REFERER   = df_clean[df_clean["Click_type"]=="EQUAL_REFERER"]
df_DYNAMIC_CHANGE  = df_clean[df_clean["Click_type"]=="DYNAMIC_CHANGE"]
df_AJAX            = df_clean[df_clean["Click_type"]=="AJAX"]

df_list = [df_TABCHANGE, df_REFRESH, df_REGULAR, df_SERVER_REDIRECT, df_EQUAL_REFERER, df_DYNAMIC_CHANGE, df_AJAX]


# In[212]:

df_list[0].head()


# In[214]:

by_date_list = [] #Group everying to daily totals
for df_i in df_list:
    by_date_list += [(df_i.groupby(["Date", "Click_type","Visitor_type"]).agg({'Count':'sum', 'Sample':'mean'})).reset_index()]


# In[215]:

by_date_list[0].head()


# In[163]:

def tsa_by_date(by_date_df):
    from statsmodels.tsa.seasonal import seasonal_decompose

    copy = by_date_df.copy()

    copy_a = copy[copy["Visitor_type"]=="visitors-total"].reset_index()
    copy_b = copy[copy["Visitor_type"]=="visitors-unique"].reset_index()

    copy_a["Ratio"] = copy_a["Count"]/copy_a["Sample"]
    copy_b["Ratio"] = copy_b["Count"]/copy_b["Sample"]

    a = np.array(copy_a["Ratio"])
    b = np.array(copy_b["Ratio"])

    model_a = seasonal_decompose(a, model="additive", filt=None, freq=7, two_sided=False)
    model_b = seasonal_decompose(b, model="additive", filt=None, freq=7, two_sided=False)

    copy_a["Norm_res"] = np.absolute(model_a.resid/a) #normalize: divide residual_i by model value at i
    copy_b["Norm_res"] = np.absolute(model_b.resid/b)

    std_a = copy_a["Norm_res"].std() #used to plot, std vs volume
    std_b = copy_b["Norm_res"].std()
    
    copy_a["Res/std"] = copy_a["Norm_res"]/std_a #these are scores for each site
    copy_b["Res/std"] = copy_b["Norm_res"]/std_b
    
    return_df = pd.concat([copy_a, copy_b], ignore_index=True).iloc[:,1:]
    
    return((return_df, std_a, std_b))
  
    #return_df = pd.merge(copy_a[["Date", "Norm_res_tot"]], copy_b[["Date", "Norm_res_uni"]], on="Date", how="left")
    #return_df["Cutoff_tot"] = cutoff_a
    #return_df["Cutoff_uni"] = cutoff_b
    #return_df["Count_tot"] = copy_a["Count"]
    #return_df["Count_uni"] = copy_b["Count"]
    #return_df["Sample"] = copy_b["Sample"]
    #return_df["Ratio_tot"] = copy_a["Ratio_tot"]
    #return_df["Ratio_uni"] = copy_b["Ratio_uni"]
    #return(return_df)


# In[167]:

date_scores_list = []
std_tot_list = []
std_uni_list = []
for df_i in by_date_list:
    temp = tsa_by_date(df_i)
    date_scores_list += [temp[0].dropna()]
    std_tot_list += [temp[1]]
    std_uni_list += [temp[2]]


# In[178]:

my_bins = list(np.arange(0, 5.5, 0.5))
i = 0
for df_i in date_scores_list:
    plt.hist(df_i["Res/std"][df_i["Visitor_type"] == "visitors-total"])
    plt.title("Dist of Res/Std_res: " + titles[i])
    i += 1
    plt.show()


# In[180]:

def tsa_by_site(df_by_site):
    copy = df_by_site.copy()
    req_len = len(copy["Date"].unique())
    unique_sites = copy["Site"].unique()
    frames = []
    my_dict = {} #stores std of residuals for tot and uni, key is domain name

    for site in unique_sites:
        copy_subset = copy[copy["Site"] == site]
        if len(copy_subset) < req_len:
            continue

        copy_subset_a = copy_subset[copy_subset["Visitor_type"]=="visitors-total"].reset_index()
        copy_subset_b = copy_subset[copy_subset["Visitor_type"]=="visitors-unique"].reset_index()

        copy_subset_a["Ratio"] = copy_subset_a["Count"]/copy_subset_a["Sample"]
        copy_subset_b["Ratio"] = copy_subset_b["Count"]/copy_subset_b["Sample"]

        a = np.array(copy_subset_a["Ratio"])
        b = np.array(copy_subset_b["Ratio"])

        model_a = seasonal_decompose(a, model="additive", filt=None, freq=7, two_sided=False)
        model_b = seasonal_decompose(b, model="additive", filt=None, freq=7, two_sided=False)

        copy_subset_a["Norm_res"] = np.absolute(model_a.resid/a)
        copy_subset_b["Norm_res"] = np.absolute(model_b.resid/b)
        
        std_a = copy_subset_a["Norm_res"].std()
        std_b = copy_subset_b["Norm_res"].std()
        
        copy_subset_a["Res/std"] = copy_subset_a["Norm_res"]/std_a
        copy_subset_b["Res/std"] = copy_subset_b["Norm_res"]/std_b
        

        frames += [pd.concat([copy_subset_a, copy_subset_b], ignore_index=True).iloc[:,1:]]
        my_dict[site] = (std_a, std_b, np.nanmean(a), np.nanmean(b))

    return_df = pd.concat(frames)
    return((return_df, my_dict))


# In[182]:

site_scores_list = []
dict_list = []
for df_i in df_list:
    temp = tsa_by_site(df_i)
    site_scores_list += [temp[0]]
    dict_list += [temp[1]]
    


# In[188]:

Site = []
std_tot = []
mean_tot = []
std_uni = []
mean_uni = []

for site in dict_list[0]:
    Site.append(site)
    std_tot.append(dict_list[0][site][0])
    std_uni.append(dict_list[0][site][1])
    mean_tot.append(dict_list[0][site][2])
    mean_tot.append(dict_list[0][site][3])


# In[227]:

temp = pd.DataFrame.from_dict(dict_list[0], orient='index')
temp.columns = ["std_of_res_tot", "std_of_res_uni", "mean_ratio_tot", "mean_ratio_uni"]


# In[230]:

temp.head(7400)


# In[219]:

plt.scatter(temp["mean_ratio_tot"], temp["std_of_res_tot"], s = 2)
plt.ylim(ymax=5)
plt.xlim(xmax=0.5)


# In[ ]:



