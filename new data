
# coding: utf-8

# In[166]:

get_ipython().magic('matplotlib inline')
import matplotlib.pyplot as plt
import matplotlib
matplotlib.style.use('ggplot')
import statsmodels as sm
from scipy.spatial.distance import cdist
import pandas as pd
from pandas import concat
from datetime import datetime
import glob
import os as os
import time
import numpy as np
from statsmodels.formula.api import ols
import seaborn as sb


# In[12]:

def tsa_by_date(by_date_df):
    from statsmodels.tsa.seasonal import seasonal_decompose

    copy = by_date_df.copy()

    copy_a = copy[copy["Visitor_type"]=="visitors-total"].reset_index()
    copy_b = copy[copy["Visitor_type"]=="visitors-unique"].reset_index()

    copy_a["Ratio"] = copy_a["Count"]/copy_a["Sample"]
    copy_b["Ratio"] = copy_b["Count"]/copy_b["Sample"]

    a = np.array(copy_a["Ratio"])
    b = np.array(copy_b["Ratio"])

    model_a = seasonal_decompose(a, model="additive", filt=None, freq=7, two_sided=False)
    model_b = seasonal_decompose(b, model="additive", filt=None, freq=7, two_sided=False)

    copy_a["Norm_res"] = np.absolute(model_a.resid/a) #normalize: divide residual_i by model value at i
    copy_b["Norm_res"] = np.absolute(model_b.resid/b)

    std_a = copy_a["Norm_res"].std() #used to plot, std vs volume
    std_b = copy_b["Norm_res"].std()
    
    copy_a["Res/std"] = copy_a["Norm_res"]/std_a #these are scores for each site
    copy_b["Res/std"] = copy_b["Norm_res"]/std_b
    
    return_df = pd.concat([copy_a, copy_b], ignore_index=True).iloc[:,1:]
    
    return((return_df, std_a, std_b))


# In[14]:

def tsa_by_site(df_by_site):
    from statsmodels.tsa.seasonal import seasonal_decompose
    
    copy = df_by_site.copy()
    req_len = len(copy["Date"].unique())
    unique_sites = copy["Site"].unique()
    frames = []
    my_dict = {} #stores std of residuals for tot and uni, key is domain name

    for site in unique_sites:
        copy_subset = copy[copy["Site"] == site]
        if len(copy_subset) < req_len:
            continue

        copy_subset_a = copy_subset[copy_subset["Visitor_type"]=="visitors-total"].reset_index()
        copy_subset_b = copy_subset[copy_subset["Visitor_type"]=="visitors-unique"].reset_index()

        copy_subset_a["Ratio"] = copy_subset_a["Count"]/copy_subset_a["Sample"]
        copy_subset_b["Ratio"] = copy_subset_b["Count"]/copy_subset_b["Sample"]

        a = np.array(copy_subset_a["Ratio"])
        b = np.array(copy_subset_b["Ratio"])

        model_a = seasonal_decompose(a, model="additive", filt=None, freq=7, two_sided=False)
        model_b = seasonal_decompose(b, model="additive", filt=None, freq=7, two_sided=False)

        copy_subset_a["Norm_res"] = np.absolute(model_a.resid/a)
        copy_subset_b["Norm_res"] = np.absolute(model_b.resid/b)
        
        std_a = copy_subset_a["Norm_res"].std()
        std_b = copy_subset_b["Norm_res"].std()
        
        copy_subset_a["Res/std"] = copy_subset_a["Norm_res"]/std_a
        copy_subset_b["Res/std"] = copy_subset_b["Norm_res"]/std_b
        

        frames += [pd.concat([copy_subset_a, copy_subset_b], ignore_index=True).iloc[:,1:]]
        my_dict[site] = (std_a, std_b, np.nanmean(a), np.nanmean(b))

    return_df = pd.concat(frames)
    return((return_df, my_dict))


# In[3]:

path = r'C:\new_data'

all_files = glob.glob(os.path.join(path, "*"))

types = {"Date":'category',
        "Site":'category',
        "URL":'category', 
        "Country":'category', 
        "Platform":'category',
        "Click_type": 'category',
        "Type":'category',
        "Count":'float32', 
        "Sample":'float32'}

df = pd.concat([pd.read_csv(f,
                            sep='/',
                            names = ["Date", "Site", "URL", "Country", "Platform", "Click_type", "Visitor_type", "Count", "Sample"],
                            dtype = types) for f in all_files], 
                            ignore_index=True)

df.columns = ["Date", "Site", "URL", "Country", "Platform", "Click_type", "Visitor_type", "Count", "Sample"]
df_PC = df[df.Platform == 'PC'].drop('Platform', axis=1)

df_count = (df_PC.groupby(["Date", "Site", "Click_type", "Visitor_type"])["Count"].sum()).reset_index() #aggregate Country 

df_sample = (df_PC.groupby(["Date", "Country"])["Sample"].mean()).reset_index() #groupby Date and Country using mean because given country and date, Sample values are same
df_sample = (df_sample.groupby(["Date"])["Sample"].sum()).reset_index() #sum over all Country to get daily Sample

df_clean = pd.merge(df_count, df_sample, on="Date", how="left")


# In[388]:

#FOLLOWING ANALYSIS AGGREGATES CLICKTYPE
f = {'Count':'sum', 'Sample':'mean'}
df_agg = (df_clean.groupby(["Date", "Site", "Visitor_type"]).agg(f)).reset_index()

date_site_agg = a[0]
date_site_agg_dict = a[1]
    
temp = pd.DataFrame.from_dict(date_site_agg_dict, orient='index')
temp.reset_index(level=0, inplace=True)
temp.columns = ["Site", "std_of_res_tot", "std_of_res_uni", "mean_ratio_tot", "mean_ratio_uni"]


# In[466]:

variance_cutoff = 0.2 #cutoff for normalized standard deviations (measure of size of of flucutations to volume).
                      #0.0 means the noise is almost non-existant.
                      #>1.0 means noise is just as large or larger than actual volume. 

temp["Variance_flag"] = "Good"
temp["Variance_flag"][temp["std_of_res_tot"]>variance_cutoff] = "High_variance"

#NOTE: We use all data points, including High_variance points to create model.
x = np.array(temp["mean_ratio_tot"])
y = np.array(temp["std_of_res_tot"])

from sklearn import linear_model   

#ln transform x and y
ln_x = np.log(x)
ln_y = np.log(y)

#linear regression on transformed variables
m,b = np.polyfit(ln_x, ln_y, deg=1)

#create model coordinates
x_plot = np.arange(min(x), max(x), 0.0001)
x_ln_plot = np.arange(min(ln_x), max(ln_x), 0.0001)

model_y = np.exp(b) * x_plot**m
transformed_model_y = m*x_ln_plot + b

#plot to confirm transformationed data linear
plt.scatter(ln_x, ln_y, s=2, color='steelblue')   
plt.plot(x_ln_plot, transformed_model_y, color='orange')   
plt.title("Total: ln(std_resid) vs ln(volume)", fontsize=16)
plt.show()   

#plot model overlaid with original data
plt.scatter(x, y, s=2, color='steelblue')   
plt.plot(x_plot, model_y, color='orange')
plt.xlim(xmin=0, xmax=0.5)
plt.ylim(ymin=0, ymax=0.5)
plt.title("Total: std_resid vs volume", fontsize=16)
plt.show()


# In[468]:

#We now set High_variance points to 0. Their extreme values result in very high euclidean distances between data and model.
variance_cutoff_index = y < variance_cutoff #array of bools to indicate whether above or below variance_cutoff
prediction = (np.exp(b) * x**m) * variance_cutoff_index
x_1 = x * variance_cutoff_index
y_1 = y * variance_cutoff_index

y_above = (prediction > y_1)*prediction + (prediction <= y_1)*y_1
                                                                   #If the prediction is below the fitted model, 
                                                                   #we set the prediction_mod=y.
                                                                   #If the prediction is above the fitted model,
                                                                   #we set prediction_mod=prediction.
                                                                   #Thus, when we calculate euclidean distances, 
                                                                   #we will not have to worry about data points below the model.
data_coord = np.array([x_1, y_above]).T
                    
prediction = np.array([x_1, prediction]).T 

distances = cdist(data_coord, prediction_coord, 'euclidean') #Each row contains the euclidean distance between the 
                                                             #data point and all points on fitted model.
min_dist = distances.min(axis=1) #Minimum distance for each row

#NOTE: Since most of the points lie below the model and hence were aritifically given distance=0,
#the cutoff percentile must be set very high or else everyting above model will be flagged.
#The correct choice of cutoff_percentile will change depending on what the choice of variance_cutoff = 0.2

cutoff_percentile = 98
cutoff = np.percentile(min_dist, cutoff_percentile) #A non-percentile approach to determining cutoff will also work.
                                                    #If a non-percentile used, can potentially include High_variance points
    
#NOTE: The exclusion of High_variance points is more clear now. High_variance points correspond to high euclidean distances.
#The abundance of High_variance points will make it very difficult to determine a suitable cutoff_percentile beacuse 
#High_variance points will consitute the majority of the top 10% of distances.

flag_coord = np.array([(min_dist > cutoff)*x_1, (min_dist > cutoff)*y_1]).T

plt.figure(num=None, figsize=(6, 6), dpi=80, facecolor='w', edgecolor='k')
plt.scatter(x_1, y_1, s=2, color='steelblue', label="good") 
plt.scatter(flag_coord[:,0], flag_coord[:,1], s=2, color='red', label="high relative variance")
plt.plot(x_plot, model_y, linewidth=1, color='orange', label="model")
plt.scatter(temp["mean_ratio_tot"][temp["Variance_flag"]=="High_variance"], 
            temp["std_of_res_tot"][temp["Variance_flag"]=="High_variance"], s=2, color='green', label="high variance")
plt.title("Total: Flagging sites")
plt.legend(loc='upper right')
plt.xlim(xmin=0, xmax=0.5)
plt.ylim(ymin=0, ymax=0.5)
plt.show()

#it's hard to see the model at x=0, but numerical inspection will show that it is very close to data


# In[462]:

temp["Relative_variance_flag"] = "Good"
temp["Relative_variance_flag"][min_dist > cutoff] = "High_relative_variance"

site_table = temp[["Site", "Variance_flag", "Relative_variance_flag"]].copy() #THIS TABLE CONTAINS CLASSIFICATION FOR ALL SITES


# In[5]:

#FOLLOWING ANALYSIS FOR EACH CLICKTYPE
titles = ["TABCHANGE", "REFRESH", "REGULAR", "SERVER_REDIRECT", "EQUAL_REFERER", "DYNAMIC_CHANGE", "AJAX"]
df_TABCHANGE       = df_clean[df_clean["Click_type"]=="TABCHANGE"]
df_REFRESH         = df_clean[df_clean["Click_type"]=="REFRESH"]
df_REGULAR         = df_clean[df_clean["Click_type"]=="REGULAR"]
df_SERVER_REDIRECT = df_clean[df_clean["Click_type"]=="SERVER_REDIRECT"]
df_EQUAL_REFERER   = df_clean[df_clean["Click_type"]=="EQUAL_REFERER"]
df_DYNAMIC_CHANGE  = df_clean[df_clean["Click_type"]=="DYNAMIC_CHANGE"]
df_AJAX            = df_clean[df_clean["Click_type"]=="AJAX"]

df_list = [df_TABCHANGE, df_REFRESH, df_REGULAR, df_SERVER_REDIRECT, df_EQUAL_REFERER, df_DYNAMIC_CHANGE, df_AJAX]


# In[6]:

#Group everying to daily totals for each clicktype
by_date_list = []
for df_i in df_list:
    by_date_list += [(df_i.groupby(["Date", "Click_type","Visitor_type"]).agg({'Count':'sum', 'Sample':'mean'})).reset_index()]
    
date_scores_list = []
std_tot_list = []
std_uni_list = []
for df_i in by_date_list:
    temp = tsa_by_date(df_i)
    date_scores_list += [temp[0].dropna()]
    std_tot_list += [temp[1]]
    std_uni_list += [temp[2]]


# In[15]:

#Group by date and site for each clicktype
site_scores_list = []
dict_list = []
for df_i in df_list:
    temp = tsa_by_site(df_i)
    site_scores_list += [temp[0]]
    dict_list += [temp[1]]

Site = []
std_tot = []
mean_tot = []
std_uni = []
mean_uni = []

for site in dict_list[0]:
    Site.append(site)
    std_tot.append(dict_list[0][site][0])
    std_uni.append(dict_list[0][site][1])
    mean_tot.append(dict_list[0][site][2])
    mean_tot.append(dict_list[0][site][3])


# In[346]:

#RAPHAEL THIS IS WHERE THE FUNCTIONING CODE STOPS, FROM HERE YOU SHOULD APPLY THE CLASSIFICATION METHOD I'VE WRITTEN SO FAR AND
#PUT IT INTO A FOR LOOP TO RUN OVER THE LISTS OF DATA THAT HAVE BEEN SEPARATED BY CLICK_TYPE.
#ALSO, EVEN THOUGH THERE'S DATA FOR DAILY TOTALS FOR EACH CLICKTYPE, DON'T WORRY ABOUT TT FOR NOW. JUST GET THE LOOP WORKING FOR
#site_scores_list

temp = pd.DataFrame.from_dict(dict_list[0], orient='index')
temp.reset_index(level=0, inplace=True)
temp.columns = ["Site", "std_of_res_tot", "std_of_res_uni", "mean_ratio_tot", "mean_ratio_uni"]

